---
layout: post
title: 逻辑回归作业
keyword: note
tags: MUST
date: 2022-01-08 15:20:26 +800
category: Note
---

# 逻辑回归作业

## 1.引例

徐同学的公司里有几名程序员，有一天公司组织了一个职业技能考试，测试大家的编程能力。拿到考试结果后，徐同学发现，编程能力和员工的头发发量有着显著的相关性，于是，他借助刚学会的线性回归做了一个分析：

```python
data = pd.DataFrame(
                    {'name':['赵工','钱工','孙工','李工','周工','吴工','郑工','王工','徐工'],
                     'hair':[0,1,2,3,4,5,6,7,8],
                     'score':[100,95,92,85,88,77,74,67,54]
                    }
                    )
```

```python
fig,ax = plt.subplots()
sns.regplot(x='hair',y='score',data=data)
plt.show()
```

![image-20211031175032455](/images/images/ml/output_3_0.png)

徐同学得出结论：他已经掌握了头发发量和编程能力的规律，并打算提交给HR作为招聘的依据。

但这时候来了一个非常厉害的长发及腰的老程序员，轻松拿下100分，于是数据集发生了变化。于是，拟合出来的直线产生了很大的偏移，这会让整个模型大大偏离了原有的范围。

```python
data.iloc[-1]=['老徐工',9,100]
```

![image-20211031175050299](/images/images/ml/output_5_0.png)

这个时候该怎么办呢？逻辑回归算法的原理当线性回归的预测结果，由于受到个别极端数值的影响而不准的时候，就可以用逻辑回归来解决。

## 2.逻辑回归

在数学中，我们通常会采取一些平滑函数，去减小这些极端值对于整体分布的影响，让整体的分布更加集中。

所谓的平滑函数，可以理解为是把线性回归预测到的具体的值，通过一个函数转化成为 0~1 的一个概率值，如下图所示。

![1](https://rajputhimanshu.files.wordpress.com/2018/03/linear_vs_logistic_regression.jpg)

虽然顶着“回归”的名号，但逻辑回归解决的却是实打实的分类问题。之所以取了这个名字，原因在于它来源于对线性回归算法的改进。通过引入单调可微函数将线性回归模型的连续预测值与分类任务的离散标记联系起来。当此函数取成对数函数的形式时，线性回归就演变为了逻辑回归。

在最简单的二分类问题中，分类的标记可以抽象为 0 和 1，因而线性回归中的实值输出需要映射为二进制的结果。逻辑回归中，实现这一映射是对数几率函数，也叫 Sigmoid 函数。它的优点是连续、平滑，且便于求导。

![img](https://miro.medium.com/max/700/1*a04iKNbchayCAJ7-0QlesA.png)

**如何理解逻辑回归算法？**

通过这个函数的映射，可以把某些极端值产生的影响变得非常微小，尽可能地降低它们造成误判的结果，而且，这个函数的输出是在 0~1 之间，也就是说逻辑回归的结果会输出一个事件的概率。对应到刚才的例子中，就是当 Sigmoid 函数的输出大于 0.5 的时候，预这名员工的编程能力比较强，小于 0.5 的时候，预测ta比较弱。

对此，徐同学对数据集进行了手动分类，把编程能力强的定义为“0”，其他定义为“1”。

```python
data1 = pd.DataFrame(
                    {'name':['赵工','钱工','孙工','李工','周工','吴工','郑工','王工','徐工'],
                     'hair':[0,1,2,3,4,5,6,7,8],
                     'score':[100,95,92,85,88,77,74,67,54],
                     'perform':['0','0','0','0','0','1','1','1','1']
                    }
                    )
```

得到的数据集如下：

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>hair</th>
      <th>score</th>
      <th>perform</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>赵工</td>
      <td>0</td>
      <td>100</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>钱工</td>
      <td>1</td>
      <td>95</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>孙工</td>
      <td>2</td>
      <td>92</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>李工</td>
      <td>3</td>
      <td>85</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>周工</td>
      <td>4</td>
      <td>88</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>吴工</td>
      <td>5</td>
      <td>77</td>
      <td>1</td>
    </tr>
    <tr>
      <th>6</th>
      <td>郑工</td>
      <td>6</td>
      <td>74</td>
      <td>1</td>
    </tr>
    <tr>
      <th>7</th>
      <td>王工</td>
      <td>7</td>
      <td>67</td>
      <td>1</td>
    </tr>
    <tr>
      <th>8</th>
      <td>徐工</td>
      <td>8</td>
      <td>54</td>
      <td>1</td>
    </tr>
  </tbody>
</table>



用逻辑回归进行拟合：

```python
from sklearn.linear_model import LogisticRegression #引入逻辑回归模型

data2 = np.array(data1.loc[:,['hair','score']]) #设定训练集
perform = np.array(data1.loc[:,'perform']).reshape(-1,1) #设定标签

clf = LogisticRegression().fit(data2, perform) #模型拟合
```

对于发量为9，得分100的新数据，模型预测如下：

```python
clf.predict(np.array([[9,100]]))

array(['0'], dtype=object)

```

可以看到，成功地将此人归类为0类，也就是能力强的类别。

据此画出决策边界

```python
x1= np.linspace(0,10,1000) #生成一系列三点用于画线

def x2(x1):
    return (clf.coef_[:,0] * x1 - clf.intercept_) / clf.coef_[:,1] #将模型输出的参数作用到数据点上

fig,ax = plt.subplots()
sns.scatterplot(x='hair',y='score',hue='perform',data=data1) #原数据
plt.plot(x1,x2(x1)) #模型拟合的决策边界线
plt.show()
```

![output_12_0](/images/images/ml/output_12_0.png)

## 3.逻辑回归的优缺点

因为逻辑回归在线性回归基础上，加了一个Sigmoid平滑函数，把一个预测连续值转化成了一个概率值，所以，逻辑回归也继承了线性回归算法的全部优点，也就是运算效率高、可解释性强等等优点。

不仅如此，逻辑回归因为采取了平滑函数，所以它还减小了极端值对于整体分布的影响，让整体的分布更加集中。

不过，既然继承了线性回归的优点，逻辑回归同样也存在和线性回归类似的缺点。作为分类模型，它是根据事物的线性分布转化为概率作为判断，也就是说如果一个事物不是简单的线性分布，那么它的结果也不会很理想。

## 4.总结

逻辑回归算法是机器学习领域中经典的分类算法，核心原理就是在线性回归模型基础上，把原有预测的连续值转化成一个事件的概率，用来解决分类问题。

在实际应用中，逻辑回归也可以在线性回归的基础上做进一步预测。

比如说，线性回归可以用来预测身高、销售额、房价、库存是多少，逻辑回归就可以预测身高是高了还是矮了，预测销售额提升了还是降低了，预测房价涨了还是跌了，预测库存够用还是不够用等等。

最后，逻辑回归优点可以总结为 4 点，简单易实现，运算效率高，可解释性很强，模型分布更集中，它的缺点是对于非线性分布的预测结果不会很理想。



## 引用

1. 极客时间，刘海丰，《成为AI产品经理》，逻辑回归：如何预测用户是否会购买商品？
2. 极客时间，王天一，《人工智能基础课》，机器学习,衍化至繁：逻辑回归
3. 周志华，《机器学习》。

