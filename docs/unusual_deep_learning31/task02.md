# Task02 机器学习基础

## 1 机器学习

### 1.1 基本概念

- 概念：从已知数据中获得规律，并利用规律对未知数据进行预测
- 分类：有监督学习、无监督学习、强化学习

### 1.2 数据集

- 概念：观测样本的集合，$D=\{x_1, x_2, \dots, x_n\}$，其中$d$为样本空间的维度
- 分类：训练集、验证集、测试集

## 2 误差分析

### 2.1 过拟合与欠拟合

- 误差：训练误差、泛化误差、测试误差
- 过拟合：能拟合训练样本，无法很好拟合测试样本，预防方式：减少参数、降低模型复杂度、正则化
- 欠拟合：没有很好拟合训练样本，预防方式：调整参数、增加迭代深度、用更加复杂的模型

### 2.2 泛化误差分析

- 误差公式：$$\text{Err}(\hat{f})=\text{Bias}^2 (\hat{f})+\text{Var}(\hat{f})+\sigma_{\varepsilon}^2$$其中$\varepsilon \sim N(0,\sigma_{\varepsilon})$时噪声，潜在模型为$Y=f(x) + \varepsilon$，估计模型为$\hat{f}(X)$
- 偏差：模型在样本上的期望结果与真实结果之间的差距，反映模型的拟合能力
- 方差：模型在不同训练集中得到函数的输出结果与期望结果之间的误差，反映模型的波动情况

### 2.3 交叉验证

- 基本思路：训练集划分为$K$份，其中一份作为验证集，其余$K-1$份作为训练集，模型在训练集上进行训练，在验证集上计算误差，对模型进行误差分析
- 交叉验证方法：$K$折交叉验证、留一交叉验证、$K$折重复交叉验证

## 3 有监督学习

### 3.1 基本概念

- 学习任务：已知输出空间，训练一个模型用于预测$y$的取值，使得$f(x) \cong y$
- 分类：预测值是离散值
- 回归：预测值是连续值

### 3.2 线性回归

- 概念：在样本属性和标签中找到一个线性关系，训练一个线性模型，使得预测值与样本标签的差值最小
- 线性模型一般形式：$\displaystyle f(x^{(k)}) = \sum_{i=1}^m w_i x_i^{(k)} + b$
- 优化目标：$\displaystyle (w^*,b^*) =\mathop{\arg\min} \limits_{(w,b)} \sum_{k = 1}^n(w^T x^{(k)}+b-y^{(k)})^2$

### 3.3 逻辑回归

- 概念：利用sigmoid函数，将线性回归产生的预测值映射为0到1之间
- 模型公式：
$$
g(f(x^{(k)}))=\left\{
\begin{array}{l}
1, \frac{1}{\displaystyle 1+e^{-(w^T x^{(k)}+b)}}\geq 0.5 \\ 
0, \text{otherwise}
\end{array} \right.
$$

### 3.4 支持向量机

- 基本思想：对于线性可分的数据，找到一个位于两类训练样本正中心的超平面，使得margin最大化
- 优化函数：
$$
\begin{array}{cc}
\displaystyle & \mathop{\arg \min} \limits_{(w,b)} \frac{1}{2}\sum_{i = 1}^d w_i^2 \\ 
s.t. & \forall x_i \in D, |w^T x_i + b| \geq 1
\end{array}$$

### 3.5 决策树

- 概念：基于树结构进行决策的机器学习方法，树结构中，叶子节点给出类别，内部节点代表某个属性
- 树的生成：ID3算法使用信息增益作为准则

### 3.6 随机森林

- 基本思路：创建多棵决策树，且每棵决策树之间无关联，根据多数原则决定样本分类结果
- 构建步骤：
  1. 随机有放回从训练集中抽取$m$个训练样本，创建训练集
  2. 随机选择部分特征，创建决策树
  3. 重复上述步骤，创建多棵决策树

## 4 无监督学习

### 4.1 聚类

- 基本思路：将数据分为多个类别，同一个类中对象的相似性较高，不同类中对象的差异性较大
- 常见方法：K-Means聚类、均值漂移聚类、基于密度的聚类
- K-Means聚类算法步骤：
  1. 选取$K$个对象作为初始聚类中心
  2. 计算样本数据与聚类中心的欧式距离，将数据按照聚类中心进行分类
  3. 更新聚类中心，计算每个类别所有对象的均值，将其作为聚类中心，计算目标函数的值
  4. 判断聚类中心和目标函数的值是否发生变化，没有变化则输出聚类结果，否则返回步骤2

### 4.2 降维

- 基本思路：将样本数据从维度$d$降低到更小的维度$m$，使得样本包含的信息量损失最小
- 常见方法：PCA
- 优势：
  1. 数据在低纬度下更容易处理
  2. 能显示重要特征
  3. 能够进行可视化展示
  4. 去除数据噪声，优化算法资源

## 5 总结

&emsp;&emsp;本次任务，主要介绍了机器学习基本概念，分为有监督学习、无监督学习和强化学习；对于数据集，可划分为训练集、验证集和测试集，用于模型的训练和验证；通过误差分析能更好地了解模型泛化能力，计算偏差和方差，得到模型的误差程度，采用不同的方法避免过拟合和欠拟合。有监督学习主要介绍线性回归、逻辑回归、支持向量机、决策树和随机森林，无监督学习主要介绍聚类和降维。
