# Task01 绪论与深度学习概述、数学基础

## 1 绪论与深度学习概述

### 1.1 人工智能、机器学习与深度学习

- 人工智能分类：强人工智能、弱人工智能、超级人工智能
- 机器学习分类：有监督学习、无监督学习、强化学习

### 1.2 起源与发展

- 第1阶段：提出MP神经元模型、感知器、ADLINE神经网络，并指出感知器只能解决简单的线性分类任务，无法解决XOR简单分类问题
- 第2阶段：提出Hopfiled神经网络、误差反向传播算法、CNN
- 第3阶段：提出深度学习概念，在语音识别、图像识别的应用

### 1.3 深度学习定义与分类
- 定义：采用多层网络结构对未知数据进行分类或回归
- 分类：
  1. 有监督学习：深度前馈网络、卷积神经网络、循环神经网络等
  2. 无监督学习：深度信念网、深度玻尔兹曼机、深度自编码器等

### 1.4 主要应用

- 图像处理领域：图像分类、物体检测、图像分割、图像回归
- 语音识别领域：语音识别、声纹识别、语音合成
- 自然语音处理领域：语言模型、情感分析、神经机器翻译、神经自动摘要、机器阅读理解、自然语言推理
- 综合应用：图像描述、可视回答、图像生成、视频生成

## 2 数学基础

### 2.1 矩阵论

- 张量：标量是0阶张量，矢量是1阶张量，矩阵是2阶张量，三维及以上数组称为张量
- 矩阵的秩（Rank）：矩阵向量中的极大线性无关组的数目
- 矩阵的逆：
  1. 奇异矩阵：$rank(A_{n×n})<n$
  2. 非奇异矩阵：$rank(A_{n×n})=n$
- 广义逆矩阵：如果存在矩阵$B$使得$ABA=A$，则称$B$为$A$的广义逆矩阵
- 矩阵分解：
  1. 特征分解：$A = U\Sigma U^{T}$
  2. 奇异值分解：$A = U \Sigma V^{T}$、$U^T U = V^T V = I$

### 2.2 概率统计

- 随机变量：
  1. 分类：离散随机变量、连续随机变量
  2. 概念：用概率分布来指定它的每个状态的可能性
  

- 常见的概率分布：
  1. 伯努利分布：单个二值型离散随机变量的分布，概率分布函数：$P(X=1)=p,P(X=0)=1-p$
  2. 二项分布：重复$n$次伯努利试验，概率分布函数：$P(X = k) = C_n^k p^k (1-p)^{n-k}$
  3. 均匀分布：概率密度函数：$\displaystyle p(x) = \frac{1}{b-a}, \quad a < x <b$
  4. 高斯分布：又称正态分布，概率密度函数：$\displaystyle p(x) = \frac{1}{\sqrt{2 \pi}\sigma}e^{-\frac{(x-\mu)^2}{2 \sigma^2}}$
  5. 指数分布：独立随机事件发生的时间间隔，概率密度函数：$p(x) = \lambda e^{-\lambda x} (x \geqslant 0)$

- 多变量概率分布：
  1. 条件概率：$P(X | Y)$
  2. 联合概率：$P(X, Y)$
  3. 先验概率：在事件发生前已知的概率
  4. 后验概率：基于新的信息，修正后来的先验概率，获得更接近实际情况的概率估计
  5. 全概率公式：$\displaystyle P(B) = \sum_{i = 1}^nP(A_i)P(B|A_i)$
  6. 贝叶斯公式：
  $$
  P(A_i | B) 
  = \frac{ P(B | A_i) P(A_i)}{P(B)} 
  = \frac{P(B | A_i) P(A_i)} {\displaystyle \sum_{j=1}^{n} P(A_j) P(B | A_j)}
  $$

- 常用统计量：
  1. 方差：随机变量与数学期望之间的偏离程度
  $\text{Var}(X) = E\left\{ [x-E(x)]^2 \right \} = E( x^2 ) -[E(x)]^2$
  2. 协方差：两个随机变量$X$和$Y$的总体误差
  $\text{Cov}(X,Y)=E\left\{ [x-E(x)][y-E(y)] \right\}=E \left( xy \right) - E(x)E(y)$

### 2.3 信息论

- 熵：样本集纯度指标，或样本集报班的平均信息量
$$H(X) = - \sum_{i = 1}^n P(x_i) \log_2 P(x_i)$$

- 联合熵：度量二维随机变量$XY$的不确定性
$$H(X, Y) = -\sum_{i = 1}^n \sum_{j = 1}^n P(x_i, y_j) \log_2 P(x_i, y_j)$$

- 条件熵：
$$\begin{aligned}
H(Y|X) 
&= \sum_{i = 1}^n P(x_i) H(Y|X = x_i) \\
&= -\sum_{i = 1}^n P(x_i) \sum_{j = 1}^n P(y_j | x_i) \log_2 P(y_j | x_i) \\
&= -\sum_{i = 1}^n \sum_{j = 1}^n P(x_i, y_j) \log_2 P(y_j | x_i)
\end{aligned}$$

- 互信息：
$$I(X;Y) = H(X)+H(Y)-H(X,Y)$$

- 相对熵：又称KL散度，描述两个概率分布$P$和$Q$差异，用概率分布$Q$拟合真实分布$P$时，产生的信息表达损耗
  1. 离散形式：$\displaystyle D(P||Q) = \sum P(x)\log \frac{P(x)}{Q(x)}$
  2. 连续形式：$\displaystyle D(P||Q) = \int P(x)\log \frac{P(x)}{Q(x)}$

- 交叉熵：目标与预测值之间的差距
$$\begin{aligned}
D(P||Q) 
&= \sum P(x)\log \frac{P(x)}{Q(x)} \\
&= \sum P(x)\log P(x) - \sum P(x)\log Q(x) \\
&= -H(P(x)) -\sum P(x)\log Q(x)
\end{aligned}$$

### 2.4 最优化估计

- 最小二乘估计：采用最小化误差的平方和，用于回归问题

## 3 总结

&emsp;&emsp;本次任务，主要介绍了人工智能的起源和发展，分为3个阶段，其主要应用于图像处理领域、语言识别领域、自然语言处理领域及综合应用；通过介绍矩阵论、概率统计、信息论、最优化估计等理论和概念，为学习后续课程建立数学基础。
