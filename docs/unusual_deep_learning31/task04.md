# Task04 卷积神经网络CNN

## 1 卷积

- 连续卷积：
$$
(f * g)(n)=\int_{-\infty}^{\infty} f(\tau) g(n-\tau) d \tau
$$
- 离散卷积：
$$
(f * g)(n) = \sum_{\tau = -\infty}^{\infty} f(\tau) g(n-\tau)
$$
- 卷积的输出：  
在给定一个输入信号序列$x$和滤波器$w$，卷积的输出为
$$
y_t = \sum_{k = 1}^K w_k x_{t-k+1}
$$

- 卷积结果按输出长度的分类
    1. 窄卷积：步长$T = 1$，两端不补零$P=0$，卷积后输出长度为$M-K+1$
    2. 宽卷积：步长$T = 1$，两端补零$P=K-1$，卷积后输出长度$M+K-1$ 
    3. 等宽卷积：步长$T = 1$，两端补零$P=(K-1)/2$，卷积后输出长度$M$
    
  其中，$K$为卷积核大小，$M$为原始数据长度。

- 其他卷积：转置卷积/微步卷积、空洞卷积

## 2 卷积神经网络基本原理

### 2.1 卷积层

- 二维卷积运算：二维图像为$I$，二维卷积核为$K$，卷积运算可表示为
$$
S(i, j)=(I * K)(i, j)=\sum_{m} \sum_{n} I(i-m, j-n) K(m, n)
$$

- 卷积的步长（stride）：卷积核移动的步长

- 卷积的模式：Full、Same和Valid

- 数据填充：  
  假设图像为$n \times n$维，卷积核为$f \times f$维，在进行卷积操作之前填充$p$层数据，则输出的维度：  
  输入维度：$(n - f + 1) \times (n - f + 1)$  
  输出维度：$(n + 2p - f + 1) \times (n + 2p - f + 1)$

- 感受野：卷积神经网络每一层输出的特征图上的像素点在输入图片上映射的区域大小

### 2.2 激活函数

- ReLU函数：
  1. 优点：计算速度快，输入为正数时，不存在梯度消失
  2. 缺点：可能丢掉一些特征，当输入为负数时，无法更新权重
  3. 函数公式：
  $$
   f(x)=\left\{
   \begin{array}{l} 
   0 \quad x < 0 \\ 
   x \quad x \geqslant 0
   \end{array}\right.
  $$

- Parametric ReLU
  1. 当$\alpha = 0.01$时，称为Leaky ReLU
  2. 当$\alpha$满足高斯分布，称为Randomized ReLU
  3. 优点：比sigmoid函数收敛快，解决ReLU的神经元死亡问题
  4. 缺点：需要多学习一个参数
  5. 函数公式：
  $$
  f(x)=\left\{
  \begin{array}{rc}
  \alpha x \quad x<0 \\ 
   x \quad x \geqslant 0
   \end{array}\right.
  $$

- ELU函数：
  1. 优点：能够处理函数噪声的数据，更容易收敛
  2. 缺点：计算量较大，收敛速度慢

- 注意事项：
  1. CNN在卷积层尽量不要使用Sigmoid和Tanh，将导致梯度消失；
  2. 首先选用ReLU，使用较小的学习率，以免造成神经元死亡的情况；
  3. 如果ReLU失效，考虑使用Leaky ReLU、PReLU、ELU或者Maxout，此时一般情况都可以解决。

### 2.3 池化层

- 常用池化方法：最大池化、均值池化
- 作用：
  1. 减少网络中参数计算量，防止过拟合
  2. 增强网络对输入图像中的小变形、扭曲、平移的鲁棒性
  3. 获得不由于尺寸大小而改变的等效图片表征

### 2.4 全连接层

- 作用：
  1. 对卷积层核池化层输出的特征图进行降维
  2. 将训练得到的特征表示，映射到样本标记空间中

### 2.5 输出层

- 对于分类问题，使用Softmax函数
$$
y_i = \frac{e^{z_i}}{\displaystyle \sum_{i = 1}^{n}e^{z_i}}
$$
- 对于回归问题，使用线性函数
$$
y_i = \sum_{m = 1}^{M}w_{im}x_m
$$

### 2.6 卷积神经网络的训练步骤

1. 用随机数初始化所有的卷积核、参数/权重
2. 将训练图片作为输入，执行前向步骤（卷积、ReLU、池化以及全连接层的前向传播），并计算每个类别的对应输出概率
3. 计算输出层的总误差
4. 利用反向传播算法计算相对于所有权重的误差的梯度，并用梯度下降法更新所有的卷积核、参数/权重的值，以使输出误差最小化

## 3 经典卷积神经网络

### 3.1 LeNet-5

- 应用场景：手写数字识别、英文字母识别
- 网络结构（以手写数字识别为例）：
  1. 输入层：$32 \times 32$维度的图片，即为1024个神经元
  2. C1层（卷积层）：使用6个$5 \times 5$的卷积核，可得到6个$28 \times 28$维度的特征图
  3. S2层（下采样层）：每个下抽样节点的4个输入节点求和后取平均(平均池化)，该均值乘以一个权重参数加上一个偏置参数作为激活函数的输入，激活函数的输出即是下一层节点的值
  4. C3层（卷积层）：使用16个$5 \times 5$的卷积核，可得到$16 * 6$个$(10 \times 10)$维度的特征图，相加并加上一个偏置项$b$，激活函数之后可得到16个$10 \times 10$的特征图
  5. S4层（下采样层）：进行最大池化，池化核为$2 \times 2$，可得到16个$5 * 5$的特征图
  6. F6层（全连接层）：84个节点
  7. 输出层：有10个节点，代表数字0~9

### 3.2 AlexNet
- 优点：
  1. 成功应用ReLU作为激活函数
  2. 使用Dropout，防止过拟合
  3. 使用重叠MaxPooling，提升特征丰富性
  4. 使用CUDA加速训练
  5. 进行数据增强，减轻过拟合

- 网络结构（以图像识别为例）：
  1. 输入层：使用大小为$224 \times 224 \times 3$图像作为输入
  2. C1层（卷积层）：使用96个大小为$11 \times 11$、卷积步长为4的卷积核，然后通过一个核大小为$3 \times 3$、步长为2的最大池化层进行数据降维采样
  3. C2层（卷积层）：使用256个大小为$5 \times 5$、卷积步长为1的卷积核，通过一个核大小为$3 \times 3$、步长为2的最大池化层再次进行数据降维采样
  4. C3、C4层（卷积层）：使用384个大小为$3 \times 3$、步长为1的卷积核进行same卷积
  5. C5（卷积层）：使用256个大小为$3 \times 3$、步长为1的卷积核进行same卷积，通过一个核大小为$3 \times 3$、步长为2的最大池化层再次进行数据降维采样
  6. F6、F7、F8（全连接层）：全连接加上Softmax分类器输出1000类的分类结果

### 3.3 VGGNet

- 优点与区别：
  1. 常用VGG-16，结构规整，具有较强的扩展性
  2. 相较于AlexNet，VGG-16网络模型中的卷积层均使用$3 \times 3$、步长为1的卷积核进行same卷积，池化层均使用$2 \times 2$、步长为2的池化层

### 3.4 Inception Net

- 特点：
  1. 深度：层数多达22层，在不同的层数添加两个loss，防止梯度消失
  2. 宽度：包含4个分支，在大小为$3 \times 3$、$5 \times 5$的卷积核进行卷积之前、MaxPooling之后，分别加上了$1 \times 1$的卷积核，降低特征图维度

### 3.5 ResNet

- 特点：解决训练误差像降低再升高的问题，解决梯度消失核梯度爆炸的问题
- 残差块：在标准神经网络基础上添加跳跃连接

### 3.6 DenseNet

- 特点：对于每一层，使用前面所有层的特征映射作为输入，并且使用其自身的特征映射作为所有后续层的输入
- 优点：缓解梯度消失现象，加强特征传播，减少参数的数量

## 4 主要应用

- 图像处理领域：图像分类，物体检测，图像分割，图像回归
- 语音识别领域：CLDNN、Google Deep CNN、IBM Deep CNN
- 自然语言处理领域：情感分析

## 5 总结

&emsp;&emsp;本次任务，主要介绍了卷积的概念、卷积神经网络基本原理、经典卷积神经网络以及主要应用。卷积神经网络的基本结构包括：卷积层、激活函数、池化层、全连接层、输出层等；激活函数可以使用ReLU、Parametric ReLU、ELU；池化层主要是减少网络中参数个数，防止过拟合；其主要的参数训练使用误差反向传播算法，用梯度下降法更新卷积核和参数/权重的值；经典卷积神经网络包括LeNet-5、AlexNet、VGGNet、Inception Net、ResNet、DenseNet；还介绍了主要应用场景，包括图像处理、语音识别、自然语言处理等领域。
