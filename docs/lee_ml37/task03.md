# Task03 误差和梯度下降

## 1 误差

### 1.1 误差的来源

- 主要来源：偏差（`bias`）和方差（`variance`）  
  
- 误差、偏差和方差的区别和联系  
  参考：https://www.zhihu.com/question/27068705  
> `Error`反映的是整个模型的准确度，`Bias`反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度，`Variance`反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。

- 评估$x$的偏差：  
无偏估计：$\displaystyle E[m]=E \left [\frac{1}{N}\sum x^n \right]=\frac{1}{N}\sum_nE[x^n]=\mu$  
$m$分布对于$\mu$的离散程度（方差）：$\displaystyle \text{Var}[m]=\frac{\sigma^2}{N}$

### 1.2 不同模型方差和偏差的影响

- 不同模型的方差影响：用比较简单的模型，方差比较小，用复杂的模型，方差就很大，离散程度较大，即简单模型受到不同训练集的影响比较小。

- 不同模型的偏差影响：用比较简单的模型，偏差比较大，用复杂的模型，偏差小。

- 偏差与方差分析：模型没有很好的训练集进行训练，偏差过大，导致欠拟合；模型在训练集上的误差小，但测试集上的误差很大，方差比较大，导致欠拟合。

### 1.3 模型选择

- 交叉验证：将训练集分为两部分，一部分作为训练集，一部分作为验证集。用训练集训练模型，然后再验证集上比较，确实出最好的模型之后（比如模型3），再用全部的训练集训练模型3，然后再用`public`的测试集进行测试。

- N-折交叉验证：将训练集分成N份，进行交叉验证步骤。

## 2 梯度下降

### 2.1 梯度下降法

- 目标：找一组参数$\theta$ ，让损失函数$L(\theta)$越小越好
- 步骤：
  1. 参数初始化
  2. 计算初始值时，各个参数$\theta_i$对$L$的偏微分
  3. 计算$\theta^0$减掉$\eta$乘上偏微分的值，得到一组新的参数

### 2.2 学习速率

- 调整学习率：通过调整学习速率，更新参数的时候，损失函数能更快的到达最低点。

- 自适应学习率：可使用如下公式慢慢减少学习率
$$\displaystyle \eta^t =\frac{\eta^t}{\sqrt{t+1}}$$
其中$t$是次数。随着次数的增加，$\eta^t$减小。

### 2.3 Adagrad算法

- 参数更新步骤：
$$
w^{t+1} \leftarrow  w^t -\frac{η^t}{\sigma^t}g^t \\
g^t =\frac{\partial L(\theta^t)}{\partial w} 
$$
其中$\sigma^t$表示之前参数的所有微分的均方根，对于每个参数都是不一样的。整理得到：
$$
w^{t+1} \leftarrow  w^t -\frac{η^t}{\sqrt{\sum_{i=0}^t (g^i)^2}}g^t
$$

- 在单参数下，梯度越大，就离最低点的距离越远。但是，在多参数情况下，该结论不一定成立，需要考虑二次微分。

### 2.4 随机梯度下降法

&emsp;&emsp;损失函数不需要处理训练集所有的数据，选取一个例子$x^n$，使用如下公式计算损失函数，并更新参数：
$$
L=(\hat y^n-(b+\sum w_ix_i^n))^2 \\
\theta^i =\theta^{i-1}- \eta \nabla L^n(\theta^{i-1}) 
$$

### 2.5 特征缩放

- 目的：避免在多参数情况下的学习率不一致问题
- 解决方法：方法非常多，举例使用归一化的方式进行特征缩放
$$
x_i^r \leftarrow \frac{x_i^r - m_i}{\sigma_i}
$$
其中$m_i$是第$i$个维度的平均数，$\sigma_i$是该维度的标准差，得到的结果是所有的维度的均值都是0，所有维度的方差都是1。

### 2.6 梯度下降的限制

- 容易陷入局部极值
- 可能卡在不是极值，但微分值是0的地方
- 可能实际中只是当微分值小于某一个数值就停下来了，但这里只是比较平缓，并不是极值点

## 3 总结

&emsp;&emsp;本次任务，主要介绍了误差的来源和梯度下降的相关理论：
- 误差主要来源于偏差和方差，误差反映整个模型的准确度，偏差表示模型的精准度，方差表示模型的稳定性
- 使用N-折交叉验证避免过拟合和欠拟合
- 使用Adagrad算法，处理多参数的梯度下降问题
- 使用特征缩放的方式，处理多参数的学习率不一致情况
- 梯度下降很容易陷入局部极值，从而影响模型的效果
