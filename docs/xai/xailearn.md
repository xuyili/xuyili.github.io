# XAI - Datawhale

* 可解释机器学习论文集：https://readpaper.com/user/collect/638623946528292864
* github repo：https://github.com/TommyZihao/zihao_course/tree/main/XAI
* SHAP官方文档：https://shap.readthedocs.io/en/latest/index.html
* LIME github repo：https://github.com/marcotcr/lime


## Task01 先导

**XAI(eXplainable Artificial Intelligence) 可解释人工智能**，力求解释深度学习的核心算法。基于深度学习人工智能的核心算法都源于黑盒模型，其生成结果在本质上是不可解释的，因此难以获得用户的信任。尤其是在风险大的应用场景，如医疗诊断、金融监管和自动驾驶。所以发展可解释人工智能(XAI)极为重要且紧迫。
基于深度学习AI 的不可解释性表现在诸多方面，有两种基本类型：

1. **原理上的不可解释性**。由于深度神经网络模型和算法通常十分复杂，加上黑盒学习的性质，AI通常无法对预测的结果给出自我解释，模型十分不透明，需要依靠第三方的解释系统或者人类专家的帮助才能看清其内部工作原理。我们可以先通过简单和直观的方式对神经网络进行事后解释，在一个神经网络结束训练后，通过各种方法从不同的角度对神经网络进行解释，尝试揭示其背后的决策机理，例如利用**可视化**、**神经网络输入单元重要性归因**等。
2. **语义上的不可解释性**。深度学习用于挖掘数据中变量之间的关联性，而数据关联性的产生机制有以下三种类型：因果、混淆和样本选择偏差。例如，一个基于深度学习神经网络的图像识别系统，把某个图像识别为狼，第一种：系统依据狼的【头部以及身体特征】，这种【解释】是具有稳定性和鲁棒性的。第二种：系统依据狼的某个【局部纹理】。第三种：系统依据【草原】而判断其为狼，后两种的结论可能是正确的，但这种依据混淆或样本选择偏差带来的虚假关联而做出的【解释】，一定是不稳定和缺乏鲁棒性的。遗憾的是，但深度学习神经网络算法通常找到的是虚假或表面的关联，而不是因果关系，这种【解释】对于研究者和开发者在内的解释受众来讲是不可接受的。要想解决这种类型的不可解释性，只能从改变深度学习模型做起。

可解释人工智能还涉及到生物医疗、金融、计算机视觉、自然语言处理及推荐系统。在高风险的应用领域对可解释人工智能提出了更高的要求。目前，以深度学习为主体的AI远没有达到可解释性的要求，因为我们这定义的【可解释性】，不仅要求模型对用户是透明的，能够解释其背后的工作原理；并且这种【解释】必须是本质的，具有稳定性和鲁棒性的。此外，如何将知识与深度学习模型相结合，或者导入因果关系，目前的工作都只是初步的尝试，还有待进一步深入。

---

在实际应用层面，通过刷海量数据的填鸭式学习得到的人工智能系统存在一系列隐患，并可能引发严重的社会问题，这造成了机器学习的如下三个应用缺陷：
1. 由于数据样本收集的局限和偏见，导致数据驱动的人工智能系统也是有偏见的，这种偏见甚至无异于人类社会中的偏见。
2. 黑盒似的深度神经网络还常常犯一些低级的，人类不可能犯的错误，表现出安全性上的潜在风险。比如对一张熊猫图片添加微小的噪声后，图片就被高置信度地认为是长臂猿。考虑到人脸识别在金融支付等场景的广泛应用，这种潜在的风险令人不寒而栗。
3. 从决策机制来看，当前对深度学习算法的分析还处于不透明的摸索阶段，尤其是拥有亿万个参数的超大规模预训练神经网络，如BERT、GPT3等，其决策过程在学术上没有清晰的说明。这种黑盒似的深度神经网络暂时无法获得人类的充分理解与信任。
![应用缺陷](https://docs-xy.oss-cn-shanghai.aliyuncs.com/xai01.PNG)

---

### 可解释性AI行业应用中的不同类型解释
| **行业** | **面向开发者** | **面向监管者** | **面向使用者** | **面向应用用户** |
|----------|--------------------------------|-------------------------|---------------------------------------------|----------------------------------------|
| **生物医疗** | 人工智能归纳的信息和数据规律 | 符合伦理和法规要求模型的高可信度模型的高透明度 | 模型表征与医学知识的联系，可视化、语义化、关系代理 | 模型输出的合理性，可理解的诊断结果 |
| **金融** | 模型假设是否满足，模型逻辑是否自洽，模型代码是否正常符合预设 | 人工智能风险的可解释性与人工智能建模的可解释性 | 模型算法的决策可解释性、算法的可追溯性、算法对于数据使用的偏见预警、算法的风险可控制性 | 应用服务对象：算法决策依据、算法公平性，程序样本来源：隐私权保护、知情权保护 |
| **电商推荐** | 推荐算法内在的运作机制，专业的数字化解释  | 视觉上可解释的推荐模型、可解释产品搜索 | 可解释的序列推荐、跨类别的可解释性推荐 | 隐私权保护、知情权保护，基于特征的解释及用户评论 |
| **城市管理** | 推荐算法内在的运作机制，专业的数学化解释 | 模型的合理性和稳定性，数据的安全性 | 可解释的位置推荐、最有价值的特征的推荐和解释 | 不同地点之间的关系的解释、推荐位置的特征词云 |
| **安防**  | 安防算法内部的运行机制、行为逻辑和决策依据 | 模型的公平性、偏见性和稳定性 | 模型的安全性和可靠性 | 隐私权保护、知情权保护 |
| **法律咨询** | 咨询算法内部的运行机制，模型代码是否正常符合预设 | 可视化的知识图谱检索、推理和决策逻辑 | 基于知识引入的模型，可靠性 | 内容的即时性、针对性和准确性 |

在前期的的可解释AI相关研究中，比较有影响力的工作包括以下几类：
1. **基于专家知识规则的符号推理解释**：MYCIN被认为是最早的可解释专家系统之一，其首创的certainty-factor模型被用于处理专家系统中规则的**不确定性问题**，并对其后发展起来的**贝叶斯网络**有启迪作用。基于专家知识规则的解释具有逻辑性强、易于被人类使用者理解的有优点。但是，该方法有两个缺点：首先，基于规则的决策往往过于简单，不能灵活处理大千世界的多样性和不确定性，在决策性能上比数据驱动的机器学习方法有所欠缺；其次，专家知识规则的制定会耗费大量的人力及时间，因而更新困难。这就使得专家系统在建立后很难适应不断变化的场景需求，很快变得僵化、落伍。
2. **基于自动生成规则的推理机制解释**：20世纪90年代以来，研究人员试图将神经网络和规则推理两种方法的优点结合起来，取长补短。这些研究主要集中在以下三个问题：第一、如何从训练好的神经网络中自动提取规则，以补充到先验知识规则库中；第二，如何根据神经网络训练结果来修订知识库中的已有规则；第三，如何在神经网络的训练中引入先验的知识库及规则来约束神经网络的表现。早期的基本思想是利用逻辑推理规则对神经网络进行一定程度的简化和约束来提升其可解释性。另外，值得一提的是，为了从数值化的神经网络模型中提取对应的逻辑推理规则，早期的研究还提出了利用**模糊逻辑单元**来模拟神经元节点的输入和输出，这样的模糊处理方式能够让基于规则的巨册更加灵活地适应各种输入和输出信息的不确定性。同时，自动化的规则提取又使得整个知识规则库的更新变得灵活、高效。
3. **机器学习模型的可解释性**：有几类模型本身被认为是具有较强可解释性的，其中，**线性回归**模型将输出值表示为各输入特征的线性加权和，学习得到的特征权值除以特征权值的标准差，可以直接解释为该特征对输出影响的重要性。**逻辑回归**模型则是在线性回归的基础上，通过**logit函数**将模型输出映射为分类预测结果的概率。另一类常用的**决策树**模型，是通过将输入特征的高维空间进行连续切分来自动拟合样本数据。这样通过学习得到的特征及其切分值解释了相应的决策规则。而通过进一步增加决策树的深度，能够得到更加复杂的组合决策规则。

## Task02 ZF-NET

### ZF-NET 论文十问

1. 论文试图解决什么问题

这篇论文采取可视化的方式，对深度神经网络的特征层和分类器采取事后解释的方法，试图解决两个问题：一是为人类研究者搞清楚为什么现在的模型能够有很好的性能。二是搞清楚这些模型如何改进。

2. 这是否是一个新的问题

这篇论文于2014年发表，论文中反卷积的思想，已经于2011年被论文的第一作者提出来了。

3. 这篇论文要验证一个什么科学假设

特征不变性。例如平移缩放等，小的变换对于模型第一层的影响很大，模型顶部特征层的影响较小。并且输出结果受到旋转的影响较大，若是对称图像，则影响较小。
模型整体的深度是很重要的，过小的深度会带来很差的结果。
迁移学习能够使模型在相近类型的数据集上有很好的泛化性。

4. 有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？

这篇论文在当时属于模型可视化的探索，现在被认为是可解释机器学习的早期研究，属于XAI在图像分类领域的研究。Matthew D. Zeiler

5. 论文中提到的解决方案之关键是什么？

反池化，反激活，反卷积

6. 论文中的实验是如何设计的？

移除网络中的某些层，通过消融实验观察结果误差。改变全连接层的大小对性能影响不大。然而，增加中间卷积层的大小可以在性能上获得有用的增益。但是增加中间卷积层，同时也扩大了完全连接的层会导致过拟合。

7. 用于定量评估的数据集是什么？代码有没有开源？

ImageNet 2012, PASCAL 2012, Caltech-101, Caltech-256

8. 论文中的实验及结果有没有很好地支持需要验证的科学假设？

论文表中的数据以及图例，很好地解释了模型对于不同特征层的关注信息不同。同时也可以支撑模型深度对结果误差的影响很重要这一观点假设。其在Table 6上的结果也表明了，迁移学习得到的预训练模型在相差较远的数据集上结果不好。

9. 这篇论文到底有什么贡献？

用神经网络的事后解释方法，很好地解释了特征层到底在做什么，模型到底在关注什么，分类结果是怎么样得到。

10. 下一步呢？有什么工作可以继续深入？

该篇论文在最后提到，这种方法同样适用到object detection，应该已经有后续工作完成了这一任务。

## Task03 CAM

### CAM 论文十问

1. 论文试图解决什么问题

深度学习，特别是卷积神经网络的弱监督定位，进而实现可解释性分析和显著性分析。

2. 这是否是一个新的问题

不是，在该领域已经有很多工作了

3. 这篇论文要验证一个什么科学假设

卷积神经网络能提取位置信息，并按特定类别展示出来。

4. 有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？
可视化卷积神经网络中间特征

基于CAM的可解释性分析、显著性分析
论文作者周博磊、南开大学程明明团队

5. 论文中提到的解决方案之关键是什么？

通过全局平均池化（GAP）层，获得指定类别对最后一层卷积层输出的每个特征图Channel的权重，进而计算CAM类激活热力图，展示指定类别在原图上重点关注的特征区域。

6. 论文中的实验是如何设计的？

ImageNet定位任务、其它视觉任务

7. 用于定量评估的数据集是什么？代码有没有开源？

ImageNet定位任务、VQA等其它视觉任务数据集
开源

8. 论文中的实验及结果有没有很好地支持需要验证的科学假设？

在各种视觉任务上，都表明了，CAM算法能够打破深度学习的黑箱子，让人工智能展示自己的注意力和学到的特征

9. 这篇论文到底有什么贡献？

打破深度学习的黑箱子，让人工智能展示自己的“注意力”和“学到的特征”，使得Machine Teaching成为可能。深入理解它，解释它、改进它，进而信赖它。知其然，也知其所以然。

10. 下一步呢？有什么工作可以继续深入？

Grad-CAM、Score-CAM、LayerCAM等一系列基于CAM的可解释性分析、显著性方法

## Task04 Grad-CAM

### Grad-CAM 论文十问

1. 论文试图解决什么问题

深度学习的可解释性分析、显著性分析

2. 这是否是一个新的问题

不是，在Grad-CAM之前，有大量对卷积神经网络学习到的特征做可视化的工作，也有CAM类激活热力图的工作

3. 这篇论文要验证一个什么科学假设

卷积神经网络能提取位置信息，并按特定类别展示出来。

4. 有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？

可视化卷积神经网络学到的特征
CAM类激活热力图

5. 论文中提到的解决方案之关键是什么？

对原生CAM（类激活热力图）改进，计算“特定类别预测分数相对于最后一层卷积层输出特征图每个元素的偏导数”，进而计算特征图每个channel对模型预测为特定类别的影响程度。
无需修改模型，无需重新训练，即可对已有卷积神经网络模型绘制特定类别的Grad-CAM热力图，展示指定类别在原图上重点关注的特征区域，并用图像分类实现弱监督定位。可推广至图像分类、图像描述、视觉问答等多个任务。

6. 论文中的实验是如何设计的？

ImageNet弱监督定位任务
人工评价
图像分类、图像描述、视觉问答、DenseCap等其它视觉任务

7. 用于定量评估的数据集是什么？代码有没有开源？

ImageNet, COCO, ILSVRC13 detection val set
开源地址：https://github.com/ramprs/grad-cam/

8. 论文中的实验及结果有没有很好地支持需要验证的科学假设？

有

9. 这篇论文到底有什么贡献？

无需修改模型，无需重新训练，即可对已有卷积神经网络模型绘制特定类别的Grad-CAM热力图，展示指定类别在原图上重点关注的特征区域，并用图像分类实现弱监督定位。可推广至图像分类、图像描述、视觉问答等多个任务。

10. 下一步呢？有什么工作可以继续深入？

Grad-CAM++
Score-CAM
LayerCAM
等一系列基于CAM的工作

## Task05 CAM Captum代码实战

同济子豪兄 - XAI - [github项目链接](https://github.com/TommyZihao/Train_Custom_Dataset/tree/main/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/6-%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7%E5%88%86%E6%9E%90%E3%80%81%E6%98%BE%E8%91%97%E6%80%A7%E5%88%86%E6%9E%90)【代码以Linux命令行为主，最好在云算力平台上运行】

CAM的可视化主要试用【torchcam】，notebook中的安装代码：

```shell
!git clone https://github.com/frgfm/torch-cam.git
!pip install -e torch-cam/.
```

#### 图像中只有一个类别时

![一个类别](https://docs-xy.oss-cn-shanghai.aliyuncs.com/xai02.jpg)

#### 图像中含有两个类别时
<div style="display:flex">
    <img src="https://docs-xy.oss-cn-shanghai.aliyuncs.com/xai03.jpg" style="width:50%">
    <img src="https://docs-xy.oss-cn-shanghai.aliyuncs.com/xai04.jpg" style="width:50%">
</div>

#### 可解释分析热力图 与 原图结合的结果

```python
from torchcam.utils import overlay_mask
result = overlay_mask(img_pil, Image.fromarray(activation_map), alpha=0.7)
```
<div style="display:flex">
    <img src="https://docs-xy.oss-cn-shanghai.aliyuncs.com/xai05.png" style="width:40%">
    <img src="https://docs-xy.oss-cn-shanghai.aliyuncs.com/xai06.jfif" style="width:50%">
</div>

> torchcam.utils.overlay_mask(img: Image, mask: Image, colormap: str = 'jet', alpha: float = 0.7) → Image [[source]](https://frgfm.github.io/torch-cam/_modules/torchcam/utils.html#overlay_mask)
> ```python
> from PIL import Image
> import matplotlib.pyplot as plt
> from torchcam.utils import overlay_mask
> img = ...
> cam = ...
> overlay = overlay_mask(img, cam)
> ```
> PARAMETERS:
> - img – background image
> - mask – mask to be overlayed in grayscale
> - colormap – colormap to be applied on the mask
> - alpha – transparency of the background image
>
> RETURNS:
>  - overlayed image
>
> RAISES:
>  - TypeError – when the arguments have invalid types
>  - ValueError – when the alpha argument has an incorrect value
>


## Task 06 LIME

### LIME的基本假设

线性模型特征需要易于理解

特征数量需要足够少，便于人类理解 

$x\in \mathbb{R}^d$表示原模型训练用到的特征，$d$维实数向量

$x'\in \{0, 1\}^{d'}$ 可解释模型训练用到的特征，$d'$维0-1向量

### trade-off

可解释性interpretability与拟合准确度fidelity的trade-off

可解释性好的模型，模型结构与原理更加简单，更易于人类理解，但是可能无法很好地拟合数据

可解释性差的模型，模型结合与原理更加复杂，不易于人类理解，但可能对数据的拟合更加准确

### LIME公式

![LIME公式](https://docs-xy.oss-cn-shanghai.aliyuncs.com/xai07.png)

### 得到可解释模型的步骤

从原始数据中提取d维可解释特征X，转换为d'维的0-1向量，并在这些0-1向量添加扰动（将部分特征的0改为1，1改为0），得到X'，在待测样本中生成邻域数据Z'，将Z'由0-1向量恢复为d维原始向量，使用原模型进行预测。用d'维的0-1向量作为特征，用原模型的预测结果作为标签，训练出一个可解释模型g。

输入d维特征，输出原模型预测结果。通过可解释模型g中的权重，分析哪些特征对模型的预测提供了较大的贡献

![步骤](https://docs-xy.oss-cn-shanghai.aliyuncs.com/xai08.png)

### Sparese Linear Explanations

使用$\pi_x(z) = exp(-D(x,z)^2/\sigma^2)$高斯核为半径，待测样本为中心画圆，计算距离来表示局部不可信度。距离越远，局部不可信度越大。

不可信度计算公式：

![不可信度计算公式](https://docs-xy.oss-cn-shanghai.aliyuncs.com/xai09.png)

## Task 07 LIME Shap代码实战

LIME : https://github.com/marcotcr/lime#tutorials-and-api

SHAP : https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html
