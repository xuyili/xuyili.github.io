# Meta Label Correction 论文阅读

## 1.文章背景介绍

训练神经网络的时候，噪声标签是个很麻烦的问题，因为非常容易导致过拟合。

本文同样也是讨论噪声标签的问题，一开始作者就阐述了近些年有些样本重新赋权的方法取得了很好的效果。

解决这个问题通常有两种做法：一个是使用co-teaching或curriculum learning的方式，从噪声数据中挑选出正确的样本；另一个就是使用重新赋权（re-weight），这种方式可以保留全部的数据。这其中有人使用了元学习的方法，

但作者认为，这种重新赋权（re-weight）的方式有局限性，它只能降低或提升权重来控制样本对学习的贡献。

另一种可以替代的方案是基于一些假设的前提来纠正噪声标签。

本文解决问题的方式并不是重新赋权，而是从另外一个角度入手：既然标签本身有错误，那么是不是可以用自动的方式把错误标签纠正过来呢？

标签的噪声有多种产生方式，比如一些并不专业的人进行标记时难免出错，还有些是基于启发式和用户交互信号（heuristics or user interaction signals）的自动标注。

纠正标签的目的，一个是可以提高干净标签的权重，另一个是把错误的标签改过来。

以往纠正错误标签的假设都基于这个假设：1.标签损坏矩阵（estimating a label corruption matrix）2.我们的模型是基于符合这个矩阵规律的数据来训练的。这个假设成立的条件比较苛刻，通常需要假设噪声标签只依赖于真实标签，与数据本身无关。

本文提出了一个从噪声数据中纠正标签的元学习方法，取名为MLC（meta label correction，元标签纠正）。把纠正标签的过程看作一个元过程（meta-process）。

利用被元学习模型所纠正后的标签来训练成一个新的预测模型。

元学习模型和常规分类模型同时进行训练，在一个双层的优化过程（bi-level optimization procedure）里。这个可以最大化模型在干净数据集上的表现，通过更正错误标签。MLC同时发挥了重新赋权和纠正标签的双重优点。与以往的纠正标签的方法相比，它不需要基于对噪声数据本身的假设，而是直接训练一个纠正模型。







### 名词解释

| 名词                | 中文（自己翻译的） | 解释                                                         | 图解 |
| ------------------- | ------------------ | ------------------------------------------------------------ | ---- |
| Uniform label noise | 均匀噪声           | 一个数据集有C个类别，真实的标签y有$\rho/C$的概率会被错误地变成另一个可能的类别$y'$，有$1-\rho$的概率保持不变。 |      |
| Flipped label noise | 翻转噪声           | 一个数据集有C个类别，真实的标签y有$\rho$的概率会被错误地变成任何其他的类别（共$C-1$个），有$1-\rho$的概率保持不变。 |      |
|                     |                    |                                                              |      |



## 2.相关工作



## 3.算法设计

### 3.1 算法描述

我们假设有两组数据：一个干净的、可信赖的小数据集，一个含有噪声标签的大数据集。为什么这样假设呢，因为花钱请专家标注数据比较贵，所以干净的数据相比于噪声的数据要小很多。这个时候，如果直接在小数据集上训练并不是最好的选择，这样很容易过拟合。而直接在大的数据集上训练也不好，模型会直接把噪声数据全部学到了。

这时候，我们通过训练一个元模型和一个主模型，前者纠正噪音标签，后者把修正后的标签拿来训练，让这2个模型互相加强。

干净数据集，我们称作$D=\{x,y\}^m$;噪声数据集，我们称作$D'=\{x,y\}^M$,m远小于M。

我们建立了一个标签纠正网络（label correction network），接受一组噪声数据及其标签作为输入，一个新的纠正后的标签作为输出。

LCN的目标是形成一个参数为$\alpha$的函数：$y_c=g_{\alpha}\left( h(x),y'\right)$，其中$y_c$是纠正后的标签，它是一个软标签。而$h(x),y'$是一组噪声标签的数据。

主模型$f$目标是生成一个参数为$w$的函数：$y=f_w(x)$。

显然，这两个方法只能各自为战。我们通过一个双层优化的方式把他们连接在一起，这个是解一个优化问题：
$$
\min_\alpha\mathrm{E}_{(x,y)\in D}l\left(y,f_{w^{*}_\alpha(x)}\right)
$$
$s.t.$  
$$
w^*_\alpha= \argmin
$$
 $$

### 3.2 代码实现

## 4.实验设计

设定了一个概率$\rho$来破坏现有的数据集，训练过程中，模型并不知道这个概率的存在，对数据集的破坏也是随机生成的。

挑选了两个最具有代表性的SOTA对手拿来对比：基于纠正标签的GLC，基于重新赋权的Meta-Weight-Net。



### 4.1 图片分类任务

| 数据集      | 元学习数据集                     | 使用的模型              | 制造翻转噪声 |
| ----------- | -------------------------------- | ----------------------- | ------------ |
| CIFAR-10    | 从中取1000张图片                 | ResNet-32               |              |
| CIFAR-100   | 从中取1000张图片                 | ResNet-32               |              |
| Clothing 1M | 使用这个数据集自带的干净数据子集 | ResNet-50（pretrained） |              |





### 4.2 文本分类任务

## 5.个人点评
