# Task02：图的基本表示和特征工程学习笔记

学习内容来自：[斯坦福大学CS224W图机器学习公开课-同济子豪兄中文精讲](https://github.com/TommyZihao/zihao_course/tree/main/CS224W#%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6cs224w%E5%9B%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%80%E8%AF%BE-%E5%90%8C%E6%B5%8E%E5%AD%90%E8%B1%AA%E5%85%84%E4%B8%AD%E6%96%87%E7%B2%BE%E8%AE%B2)

> 同济子豪兄-中文精讲视频：https://www.bilibili.com/video/BV1pR4y1S7GA
>
> 本讲介绍了图数据挖掘的常见任务、典型方法、应用场景、编程工具。图是描述大自然各种关联现象的通用语言，图无处不在。不同于传统数据分析中样本独立同分布假设，图数据自带了关联结构，需要使用专门的图神经网络进行深度学习。 本讲介绍了斯坦福CS224W公开课的课程大纲；在节点、连接、子图、全图各个层面进行图数据挖掘的典型任务，以及在蛋白质结构预测、生物医药、内容推荐、文献挖掘、社交网络、开源项目评价等领域的应用。
>

## 1.本体笔记

本讲主要是通过人工设计的特征来进行图机器学习。

### 1.1 图机器学习的几个基本任务：

**节点层面**：输入某个d维节点的向量，输出该节点是某类的概率。是一个半监督问题。

**连接层面**：

子图/全图层面

我们需要把特征写成向量，输入到机器学习模型中。

**属性特征**

节点自身有属性特征。比如银行风控用户，每个用户是一个节点，它的属性特征有：收入、学历、年龄、婚姻、工作单位、征信等。

属性特征有时是多模态的。

**连接特征**

节点与其他节点的连接关系。大概分为四类：

连接数（node degree），只看数量，不看质量。例如地铁图，连接数高的站点通常是重要的枢纽。

这里举了一个院士之间重要程度的例子，如果只看节点连接数的话，A节点和G节点都是1，特征相同，但常识上显然二者的社交圈的影响力并不在一个数量级。

![image-20230214160801207](./cs224w02/image-20230214160801207.png)

所以这时候就需要为不同的分配重要程度（node centrality），把节点的质量也展示出来。有几种不同的衡量重要的方法：
 1. 特征向量重要度。与邻接节点的重要度求和，这是一个递归的过程，最后转换成求矩阵的特质值和特征向量。特点是更加容易偏向更重要的节点。
 2. 中间状态（betweenness）重要度。看一个节点是否处在“交通咽喉，必经之地”。 
![](./cs224w02/2023-02-15-14-46-35.png)
 3. 相近（closeness）重要度。表示一个节点去哪都近 
![](./cs224w02/2023-02-15-14-52-13.png)
 4. 
聚类系数（clustering coefficient这里子豪兄通俗地说成了”抱团系数“，好评），衡量一个节点周围有多“抱团”。

![](./cs224w02/2023-02-15-14-54-23.png)

子图模式（Graphlets自己构造的子图，类型小波变换 ）
![](./cs224w02/2023-02-15-14-57-13.png)
思路：构造新的特征，使模型更容易区分出不同类型的数据（特征工程）。



**设计过程**

把节点、全图变成d维的向量（d个特征），输入到机器学习算法里，去拟合和预测。

**子豪语录**
> 看待任何事物都要有数据挖掘的视野，就好比戴上了一副眼镜，把纷纷错乱的现象都可以用数据说话，这是一种思维方式。

> Kaggle竞赛里，很多时候并不是调参，而是根据行业知识去人工构造可能有用的特征。

**连接层面**

通过已知连接，补全未知连接。
![](./cs224w02/2023-02-15-15-07-11.png)
**有两种学习的任务：**
1. 对客观静态图学习:随机删掉已有的连接，让模型去学习，是一种半监督的方法。
2. 随时间变化。使用上一个时间段的图去预测下一个时间段的连接，预测出n个连接，通过与真实的连接比较来学习。如论文的引用等。
都需要先提取连接的特征，输出成d维向量，然后输入给模型、评分。

**连接的特征分为三类：**
1. 基于两个节点的距离。最短路径长度。只看长度，忽略质量。
![](./cs224w02/2023-02-15-15-14-42.png)
2. 基于两个节点局部的连接信息。通过共同连接个数，或者交并比。或者计算Adamic-Adar index，计算共同节点的影响力。
3. 基于两个节点在全图的连接信息。

**全图层面**
提取出的特征应该要反映全图的结构特点。
类比词袋模型，把图看作文章，把节点看作单词。
可以存在孤立节点，从全图视角去看Graphlet个数。

**Graphlet Kernel**
子图匹配很消耗算力。$n^k$的复杂度。
![](./cs224w02/2023-02-16-15-19-55.png)

**Weisfeiler-Lehman Kernel**

1. 初始化：对每个节点赋初始标签。
2. 迭代：对于每个迭代轮次，对于每个节点，将它的标签与它的邻居节点的标签按一定顺序排序，然后将排序后的标签序列作为新的节点标签。
3. 输出：将每个节点的标签序列作为特征表示。
在每轮迭代中，将节点的标签与邻居节点的标签拼接起来，用于计算新的标签。这样，通过迭代多轮，每个节点的标签将考虑到它的邻居节点的标签，并形成一种具有层次结构的图表示。在每轮迭代后，可以将每个节点的新标签作为特征表示，然后用于下一轮迭代或者最终的图相似性度量。
![](./cs224w02/2023-02-16-15-26-29.png)
## 2.思考题1

1. **连接层面，存在哪些数据挖掘任务，有何应用场景？**
   
    通过图中的已知连接，去学习和预测，来补全未知连接。
2. **连接层面可以构造哪些特征？这些特征可以归为哪三类？**
   
    连接的特征分为三类：1.基于两个节点的距离。2.基于两个节点局部的连接信息。3.基于两个节点在全图的连接信息。
3. **简述Link Prediction的基本流程**
   
   有两种学习的任务：
   	1.对客观静态图学习:随机删掉已有的连接，让模型去学习，是一种半监督的方法。
   	2.随时间变化。使用上一个时间段的图去预测下一个时间段的连接，预测出n个连接，通过与真实的连接比较来学习。如论文的引用等。
   都需要先提取连接的特征，输出成d维向量，然后输入给模型、评分。
4. **A和B都知道梅西，C和D都知道同济子豪兄，请问哪对人物更容易产生社交连接。可以用哪个特征解释？**
   
    A和B的连接是梅西，这是一个公众人物，使用连接的特征来解释，这2个人的Aadmic-Adar比较弱，几乎没有联系；C和D都认识子豪兄，虽然也是公众人物，但更多出现在B站学习人工智能课程的人，这二人更容易产生联系。
5. **两个节点没有共同好友时，可以用什么特征，将连接编码为D维向量？**
   
   可以使用节点的度数特征对这个连接进行编码。
6. **简述Katz Index的算法原理**

    使用powers of the graph adjacency matrix计算节点u和节点v之间长度为k的路径个数，是$A_{uv}^k$矩阵的第u行第v列的元素。

7. **如何计算节点U和节点V之间，长度为K的路径个数**
   ![](./cs224w02/2023-02-15-15-29-02.png)
8. **为什么不直接把link两端节点的向量特征concat到一起，作为link的向量特征**
   
   在一个大规模的图中，节点数和边数很可能是非常巨大的。因此，将link两端节点的向量特征concat到一起，将会得到一个非常稠密的向量，这对于存储和计算都是非常困难的。

## 3.思考题2

1. **全图层面，存在哪些数据挖掘任务，有何应用场景？**
   
   - 图分类：对整张图进行分类，比如将社交网络中的用户进行分类，将蛋白质结构进行分类等。
   - 图生成：生成新的图，比如根据城市道路网络数据生成新的地图。
   - 图检索：根据图的某些特征进行检索，找到与之相似的其他图，比如在化学领域中搜索与某个分子结构相似的其他分子。
2. **全图层面可以构造哪些特征？**
   
   节点数量、连接数量等图的基本信息。
3. **全图层面的Graphlet，和节点层面的Graphlet，有什么区别？**
   
   在全图层面上，Graphlet通常用于描述整个图的拓扑结构，通常包括多个不同的Graphlet，以描述不同的拓扑结构模式。而在节点层面上，Graphlet则用于描述每个节点的邻域结构，通常以Graphlet频率分布的形式来表示。节点的Graphlet频率分布可以作为一种节点特征，用于图机器学习中的节点分类、聚类等任务。
4. **子图匹配，算法复杂度如何计算？**
5. **简述Weisfeiler-Lehman Kernel的算法原理**
   
6. **Weisfeiler-Lehman Kernel的词汇表（颜色表）是如何构建的？**
   
   先给所有节点分配统一的颜色，再根据节点的连接数来染上新的颜色。再对新的图重复上述步骤，再进行一次哈希，映射成不同的编号。连接结构相同的点颜色也相同。
7. **Weisfeiler-Lehman Kernel，算法复杂度是多少？**
   
   线性复杂度。
8. **Weisfeiler-Lehman Kernel和图神经网络（GNN）有什么关系？**

    GNN 可以看作是对 WL Kernel 的一个泛化，它不仅能够考虑节点标签的信息，还可以学习节点之间的特定关系，实现更加复杂的图结构分析。在 GNN 中，图的表示可以通过邻接矩阵和节点特征矩阵来进行表示，从而通过多层神经网络的计算来对节点和图进行分类、聚类等操作。

9.  **简述Kernel Methods基本原理**
    
    把两个图的向量做数量积再做点乘形成的标量称作核方法。用于对两张图做比较。
10. **为什么在Graph-level任务中，使用Kernel Methods**
    
    有了核方法衡量图与图的接近程度以后，就可以使用一些传统的机器学习方法（如SVM）进行分类了。
11. **除了Graphlet Kernel和Weisfeiler-Lehman Kernel之外，还有哪些Kernel**
    
    Random-walk Kernel，Shortest-path graph Kernel

12. **传统图机器学习和特征工程中，哪些特征用到了邻接矩阵Adjacency Matrix？**

    节点重要度可以通过邻接矩阵的每行或每列的和来计算，接近中心性和介数中心性可以通过图的最短路径算法来计算，而最短路径算法通常也需要使用邻接矩阵。
13. **如何把无向图节点、连接、全图的特征，推广到有向图？**
    
    对于有向图，节点的特征与无向图相同，但是连接的方向需要考虑。在有向图中，连接从一个节点指向另一个节点，因此在连接特征中需要区分连接的起点和终点。例如，在无向图中，可以使用连接的权重表示节点之间的相似性；在有向图中，可以分别计算出节点的出度和入度，并使用它们的比率作为节点的度中心性指标。全图的特征在有向图中也需要区分方向。例如，在无向图中，可以使用图的密度来描述节点之间的连接紧密程度；在有向图中，则需要分别计算出入度和出度的平均值来描述连接的方向和紧密程度。
14. **如何用代码实现Weisfeiler-Lehman Kernel？**

    详见子豪兄下一节视频。